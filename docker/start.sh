#!/bin/bash
# Script de inicializaÃ§Ã£o automÃ¡tica para RunPod
# Inicia DeepSeek-Coder-V2.5 + vLLM + RAG automaticamente

set -e

echo "ðŸš€ Iniciando DeepSeek-Coder-V2.5 + vLLM + RAG..."
echo "ðŸ“¦ Modelo: $MODEL_NAME"
echo "ðŸŒ Porta: $VLLM_PORT"

# FunÃ§Ã£o para log com timestamp
log() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1"
}

# Verificar GPU
log "ðŸ” Verificando GPU..."
if command -v nvidia-smi &> /dev/null; then
    nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader,nounits
    GPU_COUNT=$(nvidia-smi --list-gpus | wc -l)
    log "âœ… $GPU_COUNT GPU(s) detectada(s)"
else
    log "âš ï¸ GPU nÃ£o detectada, usando CPU"
fi

# Verificar espaÃ§o em disco
log "ðŸ’¾ Verificando espaÃ§o em disco..."
df -h /app

# Criar diretÃ³rios necessÃ¡rios
log "ðŸ“ Criando diretÃ³rios..."
mkdir -p /app/models /app/data /app/logs /app/cache

# Configurar cache do Hugging Face
export HF_HOME=/app/cache
export TRANSFORMERS_CACHE=/app/cache
export HF_DATASETS_CACHE=/app/cache

# FunÃ§Ã£o para iniciar Qdrant (RAG)
start_qdrant() {
    log "ðŸ—„ï¸ Iniciando Qdrant para RAG..."
    
    # Verificar se Qdrant estÃ¡ instalado
    if command -v qdrant &> /dev/null; then
        qdrant --config-path /app/rag/qdrant_config.yaml &
        QDRANT_PID=$!
        log "âœ… Qdrant iniciado (PID: $QDRANT_PID)"
    else
        log "âš ï¸ Qdrant nÃ£o encontrado, RAG serÃ¡ limitado"
    fi
}

# FunÃ§Ã£o para verificar se o modelo estÃ¡ disponÃ­vel
check_model() {
    log "ðŸ” Verificando modelo $MODEL_NAME..."
    
    python3 -c "
import torch
from transformers import AutoTokenizer
try:
    tokenizer = AutoTokenizer.from_pretrained('$MODEL_NAME', trust_remote_code=True)
    print('âœ… Modelo acessÃ­vel')
except Exception as e:
    print(f'âŒ Erro ao acessar modelo: {e}')
    exit(1)
"
}

# FunÃ§Ã£o para iniciar servidor vLLM
start_vllm_server() {
    log "ðŸš€ Iniciando servidor vLLM..."
    
    # Verificar se vLLM estÃ¡ disponÃ­vel
    if python3 -c "import vllm" 2>/dev/null; then
        log "âœ… vLLM disponÃ­vel"
        
        # Iniciar servidor customizado
        python3 /app/server/vllm_server.py &
        VLLM_PID=$!
        log "âœ… Servidor vLLM iniciado (PID: $VLLM_PID)"
        
        # Aguardar servidor ficar pronto
        log "â³ Aguardando servidor ficar pronto..."
        for i in {1..60}; do
            if curl -s http://localhost:$VLLM_PORT/health > /dev/null; then
                log "âœ… Servidor vLLM pronto!"
                break
            fi
            sleep 5
            log "â³ Tentativa $i/60..."
        done
        
    else
        log "âŒ vLLM nÃ£o disponÃ­vel, usando fallback"
        # Fallback para servidor simples
        python3 -c "
from models.deepseek_local import engenheiro_ia
from fastapi import FastAPI
import uvicorn

app = FastAPI()

@app.get('/health')
def health():
    return {'status': 'healthy', 'model': 'deepseek-fallback'}

@app.post('/v1/chat/completions')
def chat(request: dict):
    prompt = request.get('messages', [{}])[-1].get('content', '')
    response = engenheiro_ia.gerar_codigo_avancado(prompt)
    return {
        'choices': [{'message': {'content': response}}]
    }

uvicorn.run(app, host='0.0.0.0', port=$VLLM_PORT)
" &
        FALLBACK_PID=$!
        log "âœ… Servidor fallback iniciado (PID: $FALLBACK_PID)"
    fi
}

# FunÃ§Ã£o para mostrar informaÃ§Ãµes de conexÃ£o
show_connection_info() {
    log "ðŸ“‹ InformaÃ§Ãµes de ConexÃ£o:"
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo "ðŸŒ Endpoint vLLM: http://0.0.0.0:$VLLM_PORT"
    echo "ðŸ“ Chat Completions: http://0.0.0.0:$VLLM_PORT/v1/chat/completions"
    echo "â¤ï¸ Health Check: http://0.0.0.0:$VLLM_PORT/health"
    echo "ðŸ“š Modelos: http://0.0.0.0:$VLLM_PORT/v1/models"
    echo ""
    echo "ðŸ”— Para Continue VSCode:"
    echo "   - URL: http://localhost:$VLLM_PORT"
    echo "   - Modelo: deepseek-coder"
    echo "   - API: OpenAI Compatible"
    echo ""
    echo "ðŸ—„ï¸ RAG Qdrant: http://0.0.0.0:6333"
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
}

# FunÃ§Ã£o para monitorar processos
monitor_processes() {
    log "ðŸ‘ï¸ Monitorando processos..."
    
    while true; do
        # Verificar se vLLM ainda estÃ¡ rodando
        if ! curl -s http://localhost:$VLLM_PORT/health > /dev/null; then
            log "âŒ Servidor vLLM nÃ£o responde, reiniciando..."
            start_vllm_server
        fi
        
        sleep 30
    done
}

# FunÃ§Ã£o de limpeza ao sair
cleanup() {
    log "ðŸ§¹ Limpando processos..."
    if [ ! -z "$VLLM_PID" ]; then
        kill $VLLM_PID 2>/dev/null || true
    fi
    if [ ! -z "$QDRANT_PID" ]; then
        kill $QDRANT_PID 2>/dev/null || true
    fi
    if [ ! -z "$FALLBACK_PID" ]; then
        kill $FALLBACK_PID 2>/dev/null || true
    fi
    log "ðŸ‘‹ Shutdown completo"
}

# Configurar trap para limpeza
trap cleanup EXIT INT TERM

# Executar inicializaÃ§Ã£o
main() {
    log "ðŸŽ¯ Iniciando sequÃªncia de boot..."
    
    # 1. Verificar modelo
    check_model
    
    # 2. Iniciar RAG (opcional)
    # start_qdrant
    
    # 3. Iniciar servidor vLLM
    start_vllm_server
    
    # 4. Mostrar informaÃ§Ãµes
    show_connection_info
    
    # 5. Monitorar (loop infinito)
    monitor_processes
}

# Executar funÃ§Ã£o principal
main
