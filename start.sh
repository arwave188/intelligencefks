#!/usr/bin/env bash
# ============================================================================
# ğŸ”¥ ARSENAL DE GUERRA - SCRIPT ÃšNICO B200 180GB
# Instala + Configura + Inicia TUDO automaticamente
# Autor: FULANOKS*CODER - Arsenal de Guerra Digital
# ============================================================================

set -euo pipefail

# Cores
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

log() { echo -e "${GREEN}[$(date +'%H:%M:%S')]${NC} $1"; }
error() { echo -e "${RED}[ERROR]${NC} $1" >&2; }
warning() { echo -e "${YELLOW}[WARNING]${NC} $1"; }
info() { echo -e "${BLUE}[INFO]${NC} $1"; }

# Banner
echo -e "${RED}"
echo "============================================================================"
echo "ğŸ”¥ğŸ”¥ğŸ”¥ ARSENAL DE GUERRA - B200 180GB SUPREMO ğŸ”¥ğŸ”¥ğŸ”¥"
echo "============================================================================"
echo "ğŸ¯ INSTALA + CONFIGURA + INICIA TUDO AUTOMATICAMENTE"
echo "âš¡ OTIMIZADO PARA B200 180GB - PERFORMANCE MÃXIMA"
echo "ğŸš€ AUTOR: FULANOKS*CODER - Arsenal de Guerra Digital"
echo "============================================================================"
echo -e "${NC}"

# Detectar ambiente
if [[ -n "${RUNPOD_POD_ID:-}" ]]; then
    export IS_RUNPOD=true
    export WORKSPACE_DIR="/workspace"
    export CACHE_DIR="/workspace/cache"
    log "âœ… RunPod detectado - ID: ${RUNPOD_POD_ID}"
else
    export IS_RUNPOD=false
    export WORKSPACE_DIR="$(pwd)"
    export CACHE_DIR="$HOME/.cache"
    warning "âš ï¸ Ambiente local detectado"
fi

# Detectar GPU
if command -v nvidia-smi &> /dev/null; then
    GPU_INFO=$(nvidia-smi --query-gpu=name,memory.total --format=csv,noheader,nounits | head -1)
    GPU_NAME=$(echo "$GPU_INFO" | cut -d',' -f1 | xargs)
    GPU_MEMORY=$(echo "$GPU_INFO" | cut -d',' -f2 | xargs)
    
    log "ğŸ® GPU detectada: $GPU_NAME ($GPU_MEMORY MB VRAM)"
    
    if [[ $GPU_MEMORY -ge 170000 ]]; then
        export GPU_TIER="B200_180GB"
        log "ğŸ”¥ğŸ”¥ğŸ”¥ B200 180GB DETECTADA - CONFIGURAÃ‡ÃƒO SUPREMA! ğŸ”¥ğŸ”¥ğŸ”¥"
    elif [[ $GPU_MEMORY -ge 75000 ]]; then
        export GPU_TIER="A100_80GB"
        log "ğŸš€ A100 80GB detectada"
    else
        export GPU_TIER="OTHER"
        warning "âš ï¸ GPU com VRAM limitada"
    fi
else
    error "âŒ NVIDIA GPU nÃ£o detectada!"
    exit 1
fi

# Configurar ambiente
log "ğŸ—ï¸ Configurando ambiente..."
mkdir -p "$CACHE_DIR"/{huggingface,transformers,datasets,pip}
mkdir -p "$WORKSPACE_DIR"/{logs,data,models}

# Configurar variÃ¡veis de ambiente
export CUDA_VISIBLE_DEVICES=0
export PYTORCH_CUDA_ALLOC_CONF="max_split_size_mb:2048,expandable_segments:True"
export TOKENIZERS_PARALLELISM=false
export HF_HOME="$CACHE_DIR/huggingface"
export TRANSFORMERS_CACHE="$CACHE_DIR/transformers"
export HF_DATASETS_CACHE="$CACHE_DIR/datasets"
export PIP_CACHE_DIR="$CACHE_DIR/pip"
export HF_HUB_ENABLE_HF_TRANSFER=1

# Instalar dependÃªncias
log "ğŸ“¦ Instalando dependÃªncias..."
python3 -m pip install --upgrade pip setuptools wheel

# Instalar por etapas para evitar conflitos
info "ğŸ”¥ Instalando PyTorch..."
python3 -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

info "ğŸ¤– Instalando Transformers..."
python3 -m pip install transformers tokenizers accelerate huggingface-hub safetensors

info "ğŸŒ Instalando servidor web..."
python3 -m pip install fastapi "uvicorn[standard]" httpx sse-starlette python-multipart websockets

info "ğŸ“Š Instalando utilities..."
python3 -m pip install numpy scipy joblib threadpoolctl pydantic python-dotenv requests rich tqdm psutil

info "ğŸ“š Instalando ML..."
python3 -m pip install datasets sentence-transformers scikit-learn

# Tentar vLLM (opcional)
info "âš¡ Tentando instalar vLLM (opcional)..."
python3 -m pip install vllm || warning "âš ï¸ vLLM nÃ£o instalado (opcional)"

# Criar configuraÃ§Ã£o .env
log "âš™ï¸ Criando configuraÃ§Ã£o .env..."
cat > "$WORKSPACE_DIR/.env" << EOF
# Arsenal de Guerra - ConfiguraÃ§Ã£o B200 180GB
IS_RUNPOD=$IS_RUNPOD
WORKSPACE_DIR=$WORKSPACE_DIR
CACHE_DIR=$CACHE_DIR
GPU_TIER=$GPU_TIER

# Modelo (usando GPT-2 como base confiÃ¡vel)
OPTIMAL_MODEL=gpt2-medium
MODEL_NAME=gpt2-medium

# ConfiguraÃ§Ãµes B200
TENSOR_PARALLEL_SIZE=1
GPU_MEMORY_UTILIZATION=0.90
MAX_MODEL_LEN=8192
MAX_NUM_BATCHED_TOKENS=4096
MAX_NUM_SEQS=256

# Servidor
HOST=0.0.0.0
PORT=8000
API_PORT=8080

# Arsenal
UNCENSORED_MODE=true
DISABLE_CONTENT_FILTER=true
ALLOW_UNSAFE_CODE=true
ARSENAL_VERSION=2.5.0

# RunPod (substitua pela sua ID real)
RUNPOD_POD_ID=SEU_RUNPOD_ID
EOF

# Baixar modelo base
log "ğŸ“¥ Baixando modelo base..."
python3 -c "
from transformers import AutoTokenizer, AutoModelForCausalLM
import os

cache_dir = '$CACHE_DIR/huggingface'
model_name = 'gpt2-medium'

try:
    print('ğŸš€ Baixando GPT-2 Medium...')
    tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)
    model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=cache_dir)
    print('âœ… Modelo base baixado!')
except Exception as e:
    print(f'âš ï¸ Erro: {e}')
    # Fallback para GPT-2 bÃ¡sico
    tokenizer = AutoTokenizer.from_pretrained('gpt2', cache_dir=cache_dir)
    model = AutoModelForCausalLM.from_pretrained('gpt2', cache_dir=cache_dir)
    print('âœ… GPT-2 bÃ¡sico baixado como fallback!')
"

# Verificar se servidores existem
if [ ! -f "server/vllm_server.py" ]; then
    error "âŒ Servidor vLLM nÃ£o encontrado!"
    exit 1
fi

if [ ! -f "api/main.py" ]; then
    error "âŒ API principal nÃ£o encontrada!"
    exit 1
fi

# Iniciar servidores
log "ğŸš€ Iniciando Arsenal B200 180GB..."

# Tentar vLLM primeiro, fallback para servidor simples
info "ğŸ”¥ Tentando iniciar vLLM..."
if python3 server/vllm_server.py --help &>/dev/null; then
    python3 server/vllm_server.py &
    SERVER_PID=$!
    SERVER_TYPE="vLLM"
else
    warning "âš ï¸ vLLM nÃ£o disponÃ­vel, usando servidor simples..."
    # Criar servidor simples inline
    python3 -c "
from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
import uvicorn
import json
import time

app = FastAPI(title='Arsenal Simple Server')
app.add_middleware(CORSMiddleware, allow_origins=['*'], allow_credentials=True, allow_methods=['*'], allow_headers=['*'])

@app.get('/')
async def root():
    return {'message': 'ğŸ”¥ Arsenal de Guerra - Servidor Simples', 'status': 'running'}

@app.get('/health')
async def health():
    return {'status': 'healthy', 'service': 'arsenal-simple'}

@app.get('/v1/models')
async def models():
    return {'object': 'list', 'data': [{'id': 'arsenal-coder', 'object': 'model'}]}

@app.post('/v1/chat/completions')
async def chat(request: Request):
    data = await request.json()
    return {
        'id': f'chatcmpl-{int(time.time())}',
        'object': 'chat.completion',
        'created': int(time.time()),
        'model': 'arsenal-coder',
        'choices': [{
            'index': 0,
            'message': {'role': 'assistant', 'content': 'ğŸ”¥ Arsenal de Guerra B200 180GB ativo! Pronto para desenvolvimento sem restriÃ§Ãµes!'},
            'finish_reason': 'stop'
        }]
    }

if __name__ == '__main__':
    uvicorn.run(app, host='0.0.0.0', port=8000)
" &
    SERVER_PID=$!
    SERVER_TYPE="Simples"
fi

# Aguardar servidor
info "â³ Aguardando servidor $SERVER_TYPE..."
for i in {1..60}; do
    if curl -s http://localhost:8000/health > /dev/null; then
        log "âœ… Servidor $SERVER_TYPE pronto!"
        break
    fi
    sleep 3
done

# Iniciar API
info "ğŸŒ Iniciando Arsenal API..."
python3 api/main.py &
API_PID=$!

# Aguardar API
for i in {1..30}; do
    if curl -s http://localhost:8080/health > /dev/null; then
        log "âœ… API pronta!"
        break
    fi
    sleep 2
done

# Finalizar
echo -e "${GREEN}"
echo "============================================================================"
echo "ğŸ‰ğŸ‰ğŸ‰ ARSENAL B200 180GB OPERACIONAL! ğŸ‰ğŸ‰ğŸ‰"
echo "============================================================================"
echo -e "${WHITE}"
echo "ğŸ¯ SERVIDORES ATIVOS:"
echo "   Servidor: http://localhost:8000 ($SERVER_TYPE)"
echo "   API: http://localhost:8080"
echo "   Docs: http://localhost:8080/docs"
echo ""
echo "ğŸ§ª TESTE RÃPIDO:"
echo "   curl http://localhost:8000/health"
echo ""
echo "ğŸ’€ CONTINUE VSCODE:"
echo "   Execute no PC: python3 arsenal_auto.py"
echo "   Informe sua Pod ID quando solicitado"
echo ""
echo "ğŸ”¥ğŸ”¥ğŸ”¥ ARSENAL B200 180GB PRONTO PARA GUERRA DIGITAL! ğŸ”¥ğŸ”¥ğŸ”¥"
echo "ğŸ’€ FULANOKS*CODER - Arsenal de Guerra Digital"
echo -e "${NC}"

# Aguardar processos
wait
